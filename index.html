<!DOCTYPE html>
<html lang="en">
	<head>
	    <script src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
		<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
	    <script src="https://netdna.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

	    <!-- Global site tag (gtag.js) - Google Analytics -->
	    <!-- remove this to take out analytics for my website -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-HK5ZED8QD2"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-HK5ZED8QD2');
		</script>

		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">
		<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.css" rel="stylesheet"  type='text/css'>
		<link rel="stylesheet" href="css/style.css">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Wenjie Sun</title>
	</head>

	<body>
 		<div class="container mt-5">
		  <div class="about row mb-3 mx-auto">
		    <div class="col" align="left">
		    	<h1 class="name">Wenjie Sun</h1>
		    </div>

		  </div>
		  <div class="row mb-5 mx-auto">
		  	<div class="col-lg-3 col-md-6" align="center">
		    	<img src="img/wenjie_sun.jpg" alt="Wenjie Sun" class="img-responsive rounded mb-2" width = "100%">
		    	<div class="row email">
		    		<div class="col">
		    			 <i class="envelope fa fa-envelope fa-lg"></i> <span>wenjie1835 at gmail dot com</span> 
		    		</div>
		    	</div>
		    	<div class="row icon">
			    	<div class="col">
			    		<a href="https://scholar.google.com/citations?user=F31K7wMAAAAJ&hl=en">
			    			<i class="fa fa-graduation-cap fa-lg" aria-hidden="true"></i>
						</a>
			    		<a href="https://www.linkedin.com/in/limillicent/">
			    			<i class="fa fa-linkedin fa-lg" aria-hidden="true"></i>
						</a>
			    		<a href="https://github.com/wenjie1835">
			    			<i class="fa fa-github fa-lg" aria-hidden="true"></i>
						</a>
			    		<a href="https://twitter.com/millicent_li">
			    			<i class="fa fa-twitter-square fa-lg" aria-hidden="true"></i>
						</a>
			    	</div>
		    	</div>
		    </div>
		    <div class="col">
		    	<p>I'm a Research Engineer at <a href="https://mbzuai.ac.ae/">MBZUAI</a> where I'm advised by <a href="https://mbzuai.ac.ae/study/faculty/lijie-hu/">Lijie Hu</a>. Broadly speaking, I'm interested in any mechanisms of model behaviors. Specifically, such as training dynamics and feature learning on the mechanism level, and decision shortcuts and generalization principles on the behavioral level.</p> 
				<p><span class="bold-text">Currently, I am seeking a PhD position starting in Fall 2026 or later within these research areas. If you are interested in collaborating, please feel free to reach out—let's work on something interesting together.</span></p>

				<p>Previously, I earned a Master's degree in Electronic and Information Engineering from <a href="https://www.sustech.edu.cn/en/">SUSTech</a> in the Fall of 2025, and obtained a Bachelor's degree in Applied Chemistry from <a href="https://english.neu.edu.cn/">Northeastern University</a>  in 2022.</p>


		    	<div class="row extras">
		    		<div class="col">
		    			Links:
		    			<a href="files/WenjieCV.pdf" target="_blank">CV</a>
		    			<!-- | -->
		    		<!-- </div> -->
		    		<!-- <div class="col"> -->
		    			<!-- <a href="personal.html">Personal</a> -->
		    			<!-- | -->
		    		<!-- </div> -->
		    		<!-- <div class="col"> -->
		    			<!-- <a href="places.html">Places</a> -->
		    			<!-- | -->
		    			<!-- <a href="photography.html">Photography</a> -->
		    		</div>
		    	</div>
		    </div>
		  </div>

		  <!-- Add a divider -->
		  <hr>
		  <!-- Add a divider -->

		  <!-- This is for the news -->
		  <!-- To add a new category, copy paste a row chunk -->
		  <div class="news row mb-3 mx-auto">
		  	<div class="col">
		  		<h2 class="news">News</h2>
		  	</div>
		  </div>
<!-- Newest category -->
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		October 2025
		  	</div>
		  	<div class="col">

		  		<p>
		  			Our paper "DiagECG: Discretized ECG Tokenization for Diagnostic Reasoning" has benn accepted as a poster by AAAI 2026.
		  		</p>
		  	</div>
		  </div>
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		Aug 2025
		  	</div>
		  	<div class="col">

		  		<p>
		  			I received my master's degree from Southern University of Science and Technology.
		  		</p>
		  	</div>
		  </div>
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		May 2025
		  	</div>
		  	<div class="col">

		  		<p>
		  			New preprint on our paper, <a href="https://arxiv.org/abs/2505.22506">Sparsification and Reconstruction from the Perspective of Representation Geometry</a>, the last paper during my master's program.
		  		</p>
		  	</div>
		  </div>
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		Dec 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			Two of my journal papers have been accepted for publication in <a href="https://ieeexplore.ieee.org/abstract/document/10782990">IEEE Transactions on Artificial Intelligence</a>(CCF-B) and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0360544224039550">Energy</a>(IF=9.4), respectively! The former (<a href="https://ieeexplore.ieee.org/abstract/document/10782990"> CauseTerML</a>) is my first paper since changing my research focus. 
		  		</p>
		  	</div>
		  </div>
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		Nov 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			I am honored to be awarded the China National Scholarship!
		  		</p>
		  	</div>
		  </div>
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		Jun 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			My first journal paper was accpted by <a href="https://www.sciencedirect.com/science/article/abs/pii/S2352152X24011125">Journal of Energy Storage</a>(IF=9.8)!
		  		</p>
		  	</div>
		  </div>
		  <!-- Newest category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		January 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			Our paper, <a href="https://arxiv.org/abs/2310.15213">Function Vectors in Large Language Models</a>, was accepted to ICLR 2024!
		  		</p>
		  	</div>
		  </div>
		  <!-- First category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		May 2023
		  	</div>

		  	<div class="col">
		  		<p>
		  			Our paper, <a href="https://arxiv.org/abs/2305.06299">Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)</a>, was accepted to ACL 2023!
		  		</p>
		  	</div>
		  </div>

  		  <!-- Second category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		April 2022
		  	</div>

		  	<div class="col">
		  		<p>
		  			I was awarded a <a href="https://www.nsfgrfp.org/">2022 NSF Graduate Research Fellowship</a>. Northeastern wrote an article about it <a href="https://www.khoury.northeastern.edu/incoming-doctoral-student-millicent-li-wins-national-science-foundation-fellowship/">here</a>.
		  		</p>
		  	</div>
		  </div>

		  <!-- Third category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		August 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			Started as an AI Resident with <a href="https://ai.facebook.com/">Fundamental AI Research (FAIR)</a> at Meta in Seattle, working on natural language processing and human-computer interaction research for a year.
		  		</p>
		  	</div>
		  </div>

		  <!-- Fourth category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		May 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			Started my internship at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> working with <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann</a> on the intersection of natural language processing and healthcare!
		  		</p>
		  	</div>
		  </div>

		  <!-- Fifth category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		April 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			Excited to announce that I’ll be starting my PhD in the <a href="https://www.khoury.northeastern.edu/">Khoury College of Computer Sciences</a> at <a href="https://www.northeastern.edu/">Northeastern University</a> in Boston, fall of 2022. Thanks to everyone who has supported me on this journey thus far!
		  		</p>
		  	</div>
		  </div>

		  <!-- Sixth category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		March 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			I was awarded an Honorable Mention for the <a href="https://www.nsfgrfp.org/">2021 NSF Graduate Research Fellowship</a> competition.
		  		</p>
		  	</div>
		  </div>

		  <!-- Add a divider -->
		  <hr>
		  <!-- Add a divider -->

		  <!-- This is for publications -->
		  <!-- To add a new category, copy paste a row chunk -->
	  	<div class="col">
	  		<h2 class="pubs">Publications</h2>
	  	</div>
		  <div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2025</b></h3>
		  		<ol class="bibliography2025">
		  			<li>
		  				<div class="row">
		  					<div class="inner_row" style="margin-bottom: 30px;">
			  					<div id="verbalization">
			  						<div class="title">Do Natural Language Descriptions of Model Activations Convey Privileged Information?</div>
			  						<div class="author"><b>Millicent Li</b>, Alberto Mario Ceballos Arroyo, Giordano Rogers, Naomi Saphra, Byron C. Wallace</div>
			  						<div class="periodical">
			  							<em> arXiv, 2025</em>
										<br>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample7" aria-expanded="false" aria-controls="collapseExample7">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2509.13316" role="button">HTML</a>
										<div class="collapse" id="collapseExample7">
											<div class="card card-body">
												Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
			  			</div>
		  			</li>
		  			<li>
		  				<div class="row">
		  					<div class="inner_row" style="margin-bottom: 30px;">
	  							<div id="the-quest">
			  						<div class="title">The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability</div>
			  						<div class="author">Aaron Mueller, ... <b>Millicent Li</b> ... Yonatan Belinkov</div>
			  						<div class="periodical">
			  							<em>Computational Linguistics (CL), 2025</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample5" aria-expanded="false" aria-controls="collapseExample5">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2408.01416" role="button">HTML</a>
										<div class="collapse" id="collapseExample5">
											<div class="card card-body">
												Interpretability provides a toolset for understanding how and why neural networks behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this paper, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate depending on the goals of a given study. We argue that this framing yields a more cohesive narrative of the field, as well as actionable insights for future work. Specifically, we recommend a focus on discovering new mediators with better trade-offs between human-interpretability and compute-efficiency, and which can uncover more sophisticated abstractions from neural networks than the primarily linear mediators employed in current work. We also argue for more standardized evaluations that enable principled comparisons across mediator types, such that we can better understand when particular causal units are better suited to particular use cases.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  			<li>
		  				<div class="row">
		  					<div class="inner_row" style="margin-bottom: 30px;">

			  					<div id="multifield-adaptive-retrieval">
			  						<div class="title">Multi-Field Adaptive Retrieval</div>
			  						<div class="author"><b>Millicent Li</b>, Tongfei Chen, Benjamin Van Durme, Patrick Xia</div>
			  						<div class="periodical">
			  							<em> International Conference on Learning Representations (ICLR), 2025</em>
										<br>
										<em style="color:red">Spotlight, Top 5%</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample6" aria-expanded="false" aria-controls="collapseExample6">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2410.20056" role="button">HTML</a>
										<div class="collapse" id="collapseExample6">
											<div class="card card-body">
												Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are unstructured: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (MFAR), a flexible framework that accommodates any number of and any type of document indices on structured data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighting of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
	  	<div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2024</b></h3>
		  		<ol class="bibliography2024">
		  			<li>
		  				<div class="row">
		  					<div class="inner_row" style="margin-bottom: 30px;">
	  							<div id="function-vectors">
			  						<div class="title">Function Vectors in Large Language Models</div>
			  						<div class="author">Eric Todd, <b>Millicent Li</b>, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, David Bau</div>
			  						<div class="periodical">
			  							<em> International Conference on Learning Representations (ICLR), 2024</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample4" aria-expanded="false" aria-controls="collapseExample4">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2310.15213" role="button">HTML</a>
										<div class="collapse" id="collapseExample4">
											<div class="card card-body">
												We report the presence of a simple mechanism that represents an input-output function as a vector within autoregressive transformer language models. Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). We test the causal effects of FVs in a variety of input contexts and find that for many tasks FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble ICL. By measuring the causal effects of the FV at each layer of the network, we find that FVs do not directly perform a task through embedding arithmetic, but rather they trigger the model to perform the task using potentially nonlinear computations. Finally, we investigate the internal structure of FVs and find while that they contain information that directly encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Taken together, our findings suggest that LLMs contain internal abstractions of general-purpose functions that can be invoked in a variety of contexts.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
	  	<div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2023</b></h3>
		  		<ol class="bibliography2023">
		  			<li>
		  				<div class="row">
		  					<div class="inner_row">
			  					<div id="gpt-3-biomed-summ">
			  						<div class="title">Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)</div>
			  						<div class="author">Chantal Shaib, <b>Millicent L. Li</b>, Sebastian Joseph, Iain Marshall, Junyi Jessy Li, Byron C. Wallace</div>
			  						<div class="periodical">
			  							<em>Annual Meeting of the Association for Computational Linguistics (ACL), 2023</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample3" aria-expanded="false" aria-controls="collapseExample3">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2305.06299" role="button">HTML</a>
										<div class="collapse" id="collapseExample3">
											<div class="card card-body">
												Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine. In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision. We consider both single- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \emph{synthesize} evidence reported across a collection of articles. We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents. We release all data and annotations used in this work.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
		  <div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2022</b></h3>
		  		<ol class="bibliography2022">
		  			<li>
		  				<div class="row">
		  					<div id="review-lm-as-kbs">
		  						<div class="title">A Review on Language Models as Knowledge Bases</div>
		  						<div class="author">Badr AlKhamissi*, <b>Millicent Li*</b>, Asli Celikyilmaz^, Mona Diab^, Marjan Ghazvininejad^</div>
		  						<div class="periodical">
		  							<em>arXiv</em>
		  						</div>
		  						<div class="eq_contrib">
		  							<em>* denotes equal contribution</em>
		  						</div>
		  						<div class="eq_sup">
		  							<em>^ denotes equal supervision</em>
		  						</div>
		  						<div class="links">
									<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample2" aria-expanded="false" aria-controls="collapseExample2">
										Abstract
									</button>
									<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2204.06031" role="button">HTML</a>
									<a class="btn btn-outline-dark" href="https://bkhmsi.github.io/lms-as-kbs/" role="button">Project Website</a>
									<div class="collapse" id="collapseExample2">
										<div class="card card-body">
											Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.
										</div>
									</div>
		  						</div>
		  					</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
		  <div class="row mb-3 mx-auto bibliography">
		  	<div class="col">
		  		<h3 class="year"><b>2020</b></h3>
		  		<ol class="bibliography2020">
		  			<li>
		  				<div class="row">
		  					<div id="multichannel-photoplethysmography">
		  						<div class="title">Multi-Channel Facial Photoplethysmography Sensing</div>
		  						<div class="author">Parker S. Ruth, Jerry Cao, <b>Millicent Li</b>, Jacob E. Sunshine, Edward J. Wang, and Shwetak N. Patel</div>
		  						<div class="periodical">
		  							<em>International Conference of the IEEE Engineering in Medicine Biology Society (EMBC 2020)</em>
		  						</div>
		  						<div class="links">
									<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample" aria-expanded="false" aria-controls="collapseExample">
										Abstract
									</button>
									<a class="btn btn-outline-dark" href="https://ieeexplore.ieee.org/document/9176700" role="button">HTML</a>

									<div class="collapse" id="collapseExample">
										<div class="card card-body">
											Motivated by the need for continuous cardiovascular monitoring, we present a system for performing photoplethysmography sensing at multiple facial locations. As a proof-of-concept, our system incorporates an optical sensor array into a wearable face mask form factor for application
											in a surgical hemodynamic monitoring use case. Here we demonstrate that our design can accurately detect pulse timing by validating estimated heart rate against ground truth electrocardiogram recordings. In an experiment across 10 experimental subjects, our system achieves an error standard deviation of 2.84 beats per minute. This system shows promise for
											performing non-invasive, continuous pulse waveform recording from multiple locations on the face.
										</div>
									</div>
		  						</div>
		  					</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>

		  <!-- Add a divider -->
		  <hr>
		  <!-- Add a divider -->

	</body>

	<footer>
	    <p class="copyright">© 2024 Millicent Li. Last updated on 2025-02-19. </p>
	</footer>
